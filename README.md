# Foundation Models for Video Understanding: A Survey


# Menu

- [Image-based Video Foundation Models](#Image-based)
- [Video-based Video Foundation Models](#Video-based)
- [Universal Foundation Models](#Universal)


# Image-based
**VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners.** [arxiv, 2022]. <br>
*Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, Jiahui Yu.*<br>
 [[Paper](https://arxiv.org/pdf/2212.04979.pdf)] 

**Unmasked Teacher: Towards Training-Efficient Video Foundation Models.** [ICCV, 2023]. <br>
*Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, Yu Qiao.*<br>
 [[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Unmasked_Teacher_Towards_Training-Efficient_Video_Foundation_Models_ICCV_2023_paper.pdf)] [[Code](https://github.com/OpenGVLab/unmasked_teacher.git)]

**MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks.** [TMLR, 2023]. <br>
*Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei Zhou, Andrew Dai, Zhifeng Chen, Claire Cui, Anelia Angelova.*<br>
 [[Paper](https://openreview.net/pdf?id=FqOG4osY7C)] [[Code](https://github.com/lucidrains/MaMMUT-pytorch)]

# Video-based

# Universal
